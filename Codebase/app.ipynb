{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Causal Query Analysis System (ChromaDB + Gemini)\n",
    "\n",
    "This notebook implements a causal analysis pipeline for customer service conversations.\n",
    "\n",
    "**Workflow:**\n",
    "- Loads turn-level and conversation-level transcript data\n",
    "- Retrieves semantically similar evidence from a persistent ChromaDB vector store\n",
    "- Expands user queries and classifies query categories using Gemini\n",
    "- Generates concise causal explanations with cited transcript evidence\n",
    "- Outputs structured results (Query ID, Category, System Output, Remarks) to CSV\n",
    "\n",
    "**Key Technologies:**\n",
    "- Sentence Transformers for semantic retrieval\n",
    "- ChromaDB for persistent vector search\n",
    "- Gemini LLM for intent expansion, classification, and explanation\n",
    "\n",
    "**Output:**\n",
    "- `task1_queries.csv` containing analysis-ready results for reporting and PDF export\n"
   ],
   "id": "56e51fcb7472ed2f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from google import genai\n",
    "from google.colab import userdata\n",
    "import json\n",
    "import os\n",
    "\n",
    "turn_df = pd.read_csv(\"utterances_final.csv\")\n",
    "conv_summary_df = pd.read_csv(\"conversation_level_summary.csv\")\n",
    "\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"multi-qa-mpnet-base-dot-v1\",\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"chroma_db_turns3\")\n",
    "evidence_collection = chroma_client.get_collection(\n",
    "    name=\"evidence_turns\",\n",
    "    embedding_function=embedding_func\n",
    ")\n",
    "\n",
    "class CausalQuerySystem:\n",
    "    def __init__(self, turn_df, conv_summary_df, collection, api_key):\n",
    "        self.turn_df = turn_df\n",
    "        self.conv_summary_df = conv_summary_df\n",
    "        self.collection = collection\n",
    "        self.client = genai.Client(api_key=api_key)\n",
    "\n",
    "    def _expand_query_intent(self, user_query):\n",
    "\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert search optimizer.\n",
    "        The user is asking: \"{user_query}\"\n",
    "\n",
    "        Write a hypothetical customer service dialogue turn (1-2 sentences) that would appear in a transcript related to this issue.\n",
    "        Include keywords like 'frustrated', 'supervisor', 'delivery', 'fraud', or specific error codes if relevant.\n",
    "        Focus on the root cause.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model='gemini-2.5-flash',\n",
    "                contents=prompt\n",
    "            )\n",
    "            expanded_text = response.text.strip()\n",
    "            print(f\"Query Expanded to: {expanded_text[:100]}...\")\n",
    "            return expanded_text\n",
    "        except:\n",
    "            return user_query\n",
    "\n",
    "    def _get_category(self, user_query):\n",
    "\n",
    "\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Classify this customer service query into ONE of these categories:\n",
    "        - Escalation Analysis\n",
    "        - Fraud & Security\n",
    "        - Logistics & Delivery\n",
    "        - Billing & Payments\n",
    "        - General Support\n",
    "\n",
    "        QUERY: \"{user_query}\"\n",
    "\n",
    "        Output only the category name.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model='gemini-2.5-flash',\n",
    "                contents=prompt\n",
    "            )\n",
    "            return response.text.strip()\n",
    "        except:\n",
    "            return \"Causal Analysis\"\n",
    "\n",
    "    def query(self, user_query: str, top_k: int = 9):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üîç CAUSAL QUERY: {user_query}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        search_query = self._expand_query_intent(user_query)\n",
    "\n",
    "        results = self.collection.query(\n",
    "            query_texts=[search_query],\n",
    "            n_results=top_k\n",
    "        )\n",
    "\n",
    "        retrieved_ids = results['ids'][0]\n",
    "        retrieved_metas = results['metadatas'][0]\n",
    "        retrieved_distances = results['distances'][0]\n",
    "\n",
    "        print(f\"‚úì Found {len(retrieved_ids)} specific evidence points.\")\n",
    "\n",
    "        evidence_list = []\n",
    "        unique_transcripts = set()\n",
    "\n",
    "        for i, full_id in enumerate(retrieved_ids):\n",
    "            tid, turn_no = full_id.split('_')\n",
    "            turn_no = int(turn_no)\n",
    "            unique_transcripts.add(tid)\n",
    "\n",
    "            meta = retrieved_metas[i]\n",
    "\n",
    "            try:\n",
    "                turn_row = self.turn_df[\n",
    "                    (self.turn_df['transcript_id'] == tid) &\n",
    "                    (self.turn_df['turn_no'] == turn_no)\n",
    "                ].iloc[0]\n",
    "                text_content = turn_row['text']\n",
    "                speaker = turn_row['speaker']\n",
    "            except:\n",
    "                text_content = \"Text not found\"\n",
    "                speaker = \"Unknown\"\n",
    "\n",
    "            evidence_list.append({\n",
    "                'transcript_id': tid,\n",
    "                'turn_no': turn_no,\n",
    "                'speaker': speaker,\n",
    "                'text': text_content,\n",
    "                'outcome': meta.get('outcome', 'Unknown'),\n",
    "                'sentiment': meta.get('sentiment', 0.0),\n",
    "                'patterns': meta.get('patterns', '[]'),\n",
    "                'similarity': round(1 - retrieved_distances[i], 4)\n",
    "            })\n",
    "\n",
    "        context_docs = []\n",
    "        for tid in list(unique_transcripts)[:3]:\n",
    "            try:\n",
    "                summary = self.conv_summary_df[self.conv_summary_df['transcript_id'] == tid].iloc[0]\n",
    "\n",
    "\n",
    "                turns = self.turn_df[self.turn_df['transcript_id'] == tid].sort_values('turn_no')\n",
    "                turn_text = \"\\n\".join([f\"T{t['turn_no']} ({t['speaker']}): {t['text'][:100]}\" for _, t in turns.head(20).iterrows()])\n",
    "\n",
    "                context_docs.append(f\"\"\"\n",
    "                TRANSCRIPT {tid}:\n",
    "                - Outcome: {summary['outcome']}\n",
    "                - Reason: {summary['reason_for_call']}\n",
    "                - Excerpt:\n",
    "                {turn_text}\n",
    "                \"\"\")\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # 4. Generate Causal Explanation\n",
    "        print(f\"‚úì Generating explanation with Gemini...\")\n",
    "        explanation = self._generate_gemini_response(user_query, evidence_list, context_docs)\n",
    "\n",
    "        return {\n",
    "            'query': user_query,\n",
    "            'explanation': explanation,\n",
    "            'evidence_count': len(evidence_list),\n",
    "            'retrieved_ids': list(unique_transcripts)\n",
    "        }\n",
    "\n",
    "    def _generate_gemini_response(self, query, evidence, context):\n",
    "        # Format the evidence list for the prompt\n",
    "        evidence_str = \"\"\n",
    "        for i, ev in enumerate(evidence, 1):\n",
    "            evidence_str += f\"\"\"\n",
    "            {i}. Transcript {ev['transcript_id']} (Turn {ev['turn_no']})\n",
    "               - Speaker: {ev['speaker']}\n",
    "               - Text: \"{ev['text']}\"\n",
    "               - Sentiment: {ev['sentiment']}\n",
    "               - Detected Patterns: {ev['patterns']}\n",
    "            \"\"\"\n",
    "\n",
    "        context_str = \"\\n\".join(context)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert Causal Analysis AI. Explain WHY specific outcomes occurred using the provided evidence.\n",
    "\n",
    "USER QUERY: \"{query}\"\n",
    "\n",
    "RETRIEVED CAUSAL EVIDENCE:\n",
    "{evidence_str}\n",
    "\n",
    "CONVERSATION CONTEXT:\n",
    "{context_str}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. **ULTRA-BREVITY:** Total response MUST be under 100 words. Use basic, simple English.\n",
    "2. **STRICT QUANTITY:** You MUST provide EXACTLY 4 Causal Factors and EXACTLY 4 Recommendations.\n",
    "3. **TELEGRAPHIC STYLE:** Use very short phrases (3-5 words) for bullets. No long sentences.\n",
    "4. **CITATIONS:** Every single bullet MUST end with: ***(Source: Transcript ID, Turn No)***.\n",
    "\n",
    "FORMAT:\n",
    "## Causal Explanation\n",
    "[1-sentence summary].\n",
    "\n",
    "## Key Causal Factors\n",
    "- [Short Phrase] ***(Source: ID, Turn)***\n",
    "- [Short Phrase] ***(Source: ID, Turn)***\n",
    "- [Short Phrase] ***(Source: ID, Turn)***\n",
    "- [Short Phrase] ***(Source: ID, Turn)***\n",
    "\n",
    "## Recommendations\n",
    "1. **[Short Name]:** [Action] ***(Source: ID, Turn)***\n",
    "2. **[Short Name]:** [Action] ***(Source: ID, Turn)***\n",
    "3. **[Short Name]:** [Action] ***(Source: ID, Turn)***\n",
    "4. **[Short Name]:** [Action] ***(Source: ID, Turn)***\n",
    "\n",
    "[METADATA_START]\n",
    "GROUNDING: [1 sentence on which pattern was most important]\n",
    "CONFIDENCE: [High/Medium based on data match]\n",
    "[METADATA_END]\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model='gemini-2.5-flash',\n",
    "                contents=prompt\n",
    "            )\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            return f\"Error generating explanation: {e}\"\n",
    "\n",
    "try:\n",
    "    API_KEY = userdata.get('GEMINI_API_KEY')\n",
    "except:\n",
    "    API_KEY = input(\"Enter Gemini API Key: \")\n",
    "\n",
    "# Initialize System\n",
    "system = CausalQuerySystem(\n",
    "    turn_df=turn_df,\n",
    "    conv_summary_df=conv_summary_df,\n",
    "    collection=evidence_collection,\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "TEST_QUERIES = [\n",
    "    \"Why do customers escalate to supervisors?\",\n",
    "    \"What triggers fraud alert investigations?\",\n",
    "    \"Why do customers complain about delivery status?\",\n",
    "    \"What causes customers to call back multiple times for the same issue?\",\n",
    "    \"Why are refund requests frequently delayed or denied?\",\n",
    "    \"What agent behaviors lead to negative customer sentiment?\",\n",
    "    \"What technical issues cause login or transaction failures?\",\n",
    "    \"Why do identity verification procedures cause customer frustration?\",\n",
    "    \"What patterns lead to customers threatening to close their accounts?\",\n",
    "    \"What are the primary triggers for sudden spikes in customer anger?\"\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# Run Loop\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING BATCH PROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "for i, q in enumerate(TEST_QUERIES):\n",
    "    result = system.query(q)\n",
    "\n",
    "    full_text = result['explanation']\n",
    "\n",
    "    dynamic_category = system._get_category(q)\n",
    "\n",
    "    metadata_match = re.search(r\"\\[METADATA_START\\](.*?)\\[METADATA_END\\]\", full_text, re.DOTALL)\n",
    "    gemini_meta = metadata_match.group(1).strip() if metadata_match else \"General analysis.\"\n",
    "\n",
    "    clean_output = re.sub(r\"\\[METADATA_START\\].*?\\[METADATA_END\\]\", \"\", full_text, flags=re.DOTALL).strip()\n",
    "\n",
    "    remarks_str = (\n",
    "        f\"üîç {gemini_meta} | \"\n",
    "        f\"Model: multi-qa-mpnet-base (768-dim) | \"\n",
    "        f\"Evidence: {result['evidence_count']} turns from {len(result['retrieved_ids'])} transcripts | \"\n",
    "        f\"Data Floor: -0.954 Sentiment.\"\n",
    "    )\n",
    "\n",
    "    all_results.append({\n",
    "        \"Query_Id\": f\"Q{i+9:03d}\",\n",
    "        \"Query\": q,\n",
    "        \"Query_Category\": dynamic_category,\n",
    "        \"System_Output\": clean_output,\n",
    "        \"Remarks\": remarks_str\n",
    "    })\n",
    "\n",
    "    print(\"\\nüí° SYSTEM OUTPUT PREVIEW:\")\n",
    "    print(result['explanation'][:500] + \"...\\n\")\n",
    "\n",
    "# Save to CSV\n",
    "submission_df = pd.DataFrame(all_results)\n",
    "submission_df.to_csv(\"task1_queries.csv\", index=False)\n",
    "print(\"‚úÖ Saved 'task1_queries.csv' successfully!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Full Transcript Context Builder\n",
    "\n",
    "This helper function retrieves the complete turn-by-turn dialogue for selected transcript IDs.\n",
    "It assembles speaker-labeled conversation history in order, enabling deeper multi-turn reasoning and context-aware analysis.\n"
   ],
   "id": "4da16aa106c2b231"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_full_transcript_text(transcript_ids, turn_df):\n",
    "    \"\"\"\n",
    "    Pulls the complete dialogue history for specific transcript IDs.\n",
    "    Used for Deep-Dive reasoning in Turn 2+.\n",
    "    \"\"\"\n",
    "    context_text = \"\"\n",
    "    for tid in transcript_ids:\n",
    "        # Get all turns for this transcript, sorted\n",
    "        chat_rows = turn_df[turn_df['transcript_id'] == tid].sort_values('turn_no')\n",
    "\n",
    "        context_text += f\"\\n\\n--- TRANSCRIPT {tid} ---\\n\"\n",
    "        for _, row in chat_rows.iterrows():\n",
    "            context_text += f\"T{row['turn_no']} ({row['speaker']}): {row['text']}\\n\"\n",
    "\n",
    "    return context_text"
   ],
   "id": "73544634df2c6871"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Robust Multi-Turn Reasoning System\n",
    "\n",
    "- This class enables multi-turn, session-based causal analysis over customer service transcripts.\n",
    "- It locks relevant transcripts during the first query, preserves conversation history, and ensures all follow-up reasoning stays grounded in the same evidence across turns.\n",
    "\n",
    "**Output:**\n",
    "- `Task_2.csv` containing analysis-ready results for reporting and PDF export"
   ],
   "id": "71f05ab3ba9acf5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import userdata\n",
    "from google import genai\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class RobustMultiTurnSystem:\n",
    "    def __init__(self, task1_engine):\n",
    "        self.engine = task1_engine\n",
    "        self.sessions = {}\n",
    "\n",
    "        try:\n",
    "\n",
    "            secret_key = userdata.get('GEMINI_API_KEY')\n",
    "            if secret_key:\n",
    "                print(\"API KEY\")\n",
    "                self.engine.client = genai.Client(api_key=secret_key)\n",
    "            else:\n",
    "                print(\"No APi key\")\n",
    "        except Exception as e:\n",
    "            print(f\"No api key\")\n",
    "\n",
    "    def start_session(self, session_id):\n",
    "        self.sessions[session_id] = {\n",
    "            'history': [],\n",
    "            'evidence_lock': [],\n",
    "            'full_context_cache': \"\"\n",
    "        }\n",
    "        print(f\"Session {session_id}\")\n",
    "\n",
    "\n",
    "\n",
    "    def _get_category(self, user_query):\n",
    "        prompt = f\"\"\"\n",
    "        Classify this customer service query into ONE of these categories:\n",
    "        - Escalation Analysis\n",
    "        - Fraud & Security\n",
    "        - Logistics & Delivery\n",
    "        - Billing & Payments\n",
    "        - General Support\n",
    "\n",
    "        QUERY: \"{user_query}\"\n",
    "\n",
    "        Output only the category name.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.engine.client.models.generate_content(\n",
    "                model='gemini-2.5-flash',\n",
    "                contents=prompt\n",
    "            )\n",
    "            return response.text.strip()\n",
    "        except:\n",
    "            return \"Causal Analysis\"\n",
    "\n",
    "\n",
    "    def query(self, user_query, session_id, turn_number):\n",
    "        session = self.sessions[session_id]\n",
    "\n",
    "        if turn_number == 1:\n",
    "            print(f\"Turn 1 Retrieval for: {user_query}\")\n",
    "\n",
    "            results = self.engine.query(user_query, top_k=5)\n",
    "\n",
    "            found_ids = results['retrieved_ids']\n",
    "            session['evidence_lock'] = found_ids\n",
    "\n",
    "            session['full_context_cache'] = get_full_transcript_text(found_ids, self.engine.turn_df)\n",
    "\n",
    "            response = results['explanation']\n",
    "\n",
    "        else:\n",
    "            print(f\"Turn {turn_number} Reasoning (Locked to {len(session['evidence_lock'])} transcripts)\")\n",
    "\n",
    "            history_str = \"\"\n",
    "            for turn in session['history']:\n",
    "                history_str += f\"User: {turn['query']}\\nAI: {turn['response']}\\n\"\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "            You are an expert analyst having a conversation about specific customer service transcripts.\n",
    "\n",
    "            LOCKED EVIDENCE (Do not discuss other transcripts):\n",
    "            {session['full_context_cache']}\n",
    "\n",
    "            CONVERSATION HISTORY:\n",
    "            {history_str}\n",
    "\n",
    "            CURRENT QUESTION: \"{user_query}\"\n",
    "\n",
    "            INSTRUCTIONS:\n",
    "            1. Answer the question specifically using the LOCKED EVIDENCE provided above.\n",
    "            2. Cite specific Turn Numbers (e.g., \"In T12...\") to prove your point.\n",
    "            3. If the user asks for a comparison, compare the locked transcripts.\n",
    "            4. Keep the response under 200 words.\n",
    "            5. Inside the Reasoning you can add (Source: ID, Turn) there is 2 to 3 things must required\n",
    "            6. Inside Evidence there is 5 to 6 minimum point which support the reasoing\n",
    "            7. If you thing the (Source: ID, Turn) have nothing work don't add that\n",
    "            8. ONLY REQUIRED THINK WHICH SATASIFY THE QUERY ONELY THAT THING YOU HAVE TO SUPPLY\n",
    "            9. ***THE RULE IS STREET WHEN YOU WRITE (Source: ID, Turn) MAKE SURE ID IS 'XXXX-XXXX-XXXX-XXXX'***\n",
    "\n",
    "            Output Format:\n",
    "            ## Reasoning\n",
    "            [Answer]\n",
    "            ## Evidence\n",
    "            - [Point 1] (Source: ID, Turn)\n",
    "            - [Point 2] (Source: ID, Turn)\n",
    "            \"\"\"\n",
    "\n",
    "            try:\n",
    "                response = self.engine.client.models.generate_content(\n",
    "                    model='gemini-2.5-flash',\n",
    "                    contents=prompt\n",
    "                ).text\n",
    "            except Exception as e:\n",
    "                response = f\"Error in reasoning: {e}\"\n",
    "\n",
    "        session['history'].append({\n",
    "            'query': user_query,\n",
    "            'response': response\n",
    "        })\n",
    "\n",
    "        return response"
   ],
   "id": "61524fc57b91fead"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "\n",
    "secret_key = userdata.get('GEMINI_API_KEY')\n",
    "if secret_key:\n",
    "    print(\"API KEY\")\n",
    "else:\n",
    "    print(\"No API KEY\")\n",
    "\n",
    "system = CausalQuerySystem(\n",
    "    turn_df=turn_df,\n",
    "    conv_summary_df=conv_summary_df,\n",
    "    collection=evidence_collection,\n",
    "    api_key=secret_key\n",
    ")\n",
    "\n",
    "task2_system = RobustMultiTurnSystem(system)\n",
    "\n",
    "# 2. Define the Test Chains\n",
    "TEST_CHAINS = [\n",
    "    {\n",
    "        \"session_id\": \"S001\",\n",
    "        \"topic\": \"Escalation\",\n",
    "        \"turns\": [\n",
    "            \"Why do customers escalate to supervisors?\",\n",
    "            \"Did the agent try to de-escalate before the manager was requested?\",\n",
    "            \"What was the specific sentiment score when the customer got angry?\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"session_id\": \"S006\",\n",
    "        \"turns\": [\n",
    "            \"What product features did customers ask about most?\",\n",
    "            \"Did agents provide accurate and complete product information?\",\n",
    "            \"Were there any upselling or cross-selling attempts?\"\n",
    "         ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# 3 - Execution Loop\n",
    "task2_results = []\n",
    "\n",
    "for chain in TEST_CHAINS:\n",
    "    sid = chain['session_id']\n",
    "    task2_system.start_session(sid)\n",
    "\n",
    "    for i, q in enumerate(chain['turns'], 1):\n",
    "        print(f\"\\nProcessing {sid} - Turn {i}...\")\n",
    "\n",
    "        # Query\n",
    "        output = task2_system.query(q, sid, turn_number=i)\n",
    "        dynamic_Query_Category = task2_system._get_category(q)\n",
    "\n",
    "        # Save Result\n",
    "        task2_results.append({\n",
    "            \"Query_Id\": f\"Q{sid}_{chr(64 + i)}\",\n",
    "            \"Query_Category\": dynamic_Query_Category,\n",
    "            \"Query\": q,\n",
    "            \"System_Output\": output,\n",
    "            \"Remarks\": str(task2_system.sessions[sid]['evidence_lock'])\n",
    "        })\n",
    "\n",
    "        time.sleep(20)\n",
    "\n",
    "pd.DataFrame(task2_results).to_csv(\"task60_interactive_submission.csv\", index=False)"
   ],
   "id": "7d21f11907e0417e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset6 = pd.read_csv(\"task60_interactive_submission.csv\")\n",
    "dataset5 = pd.read_csv(\"task50_interactive_submission.csv\")\n",
    "dataset4 = pd.read_csv(\"task40_interactive_submission.csv\")\n",
    "dataset3 = pd.read_csv(\"task30_interactive_submission.csv\")\n",
    "dataset2 = pd.read_csv(\"task20_interactive_submission.csv\")\n",
    "dataset1 = pd.read_csv(\"task10_interactive_submission.csv\")\n",
    "dataset7 = pd.read_csv(\"task70_interactive_submission.csv\")"
   ],
   "id": "a8b9517c551a9641"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "final_dataset = pd.concat([\n",
    "    dataset1,\n",
    "    dataset2,\n",
    "    dataset3,\n",
    "    dataset4,\n",
    "    dataset5,\n",
    "    dataset6,\n",
    "    dataset7\n",
    "], ignore_index=True)"
   ],
   "id": "2251727c414cc057"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_sorted = final_dataset.sort_values(by='Query_Id', ascending=True)",
   "id": "efd751bcaaedc6e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_sorted.to_csv(\"Task_2.csv\", index=False)",
   "id": "ba737e1a3596ec4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**END**",
   "id": "a0d7988a33aa815f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8f39665d6c9f4e08"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
